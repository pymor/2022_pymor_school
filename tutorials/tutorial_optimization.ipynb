{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64c20cde",
   "metadata": {},
   "source": [
    "```{try_on_binder}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d15288",
   "metadata": {
    "load": "myst_code_init.py",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92dbcafb",
   "metadata": {},
   "source": [
    "# Tutorial: Model order reduction for PDE-constrained optimization problems\n",
    "\n",
    "A typical application of model order reduction for PDEs are\n",
    "PDE-constrained parameter optimization problems. These problems aim to\n",
    "find a local minimizer of an objective functional depending on an\n",
    "underlying PDE which has to be solved for all evaluations.\n",
    "A prototypical example of a PDE-constrained optimization problem can be defined\n",
    "in the following way. For a physical domain {math}`\\Omega \\subset \\mathbb{R}^d` and a\n",
    "parameter set {math}`\\mathcal{P} \\subset \\mathbb{R}^P`, we want to find\n",
    "a solution of the minimization problem\n",
    "\n",
    "```{math}\n",
    "\\min_{\\mu \\in \\mathcal{P}} J(u_{\\mu}, \\mu),  \\tag{P.a}\n",
    "```\n",
    "\n",
    "where {math}`u_{\\mu} \\in V := H^1_0(\\Omega)` is the solution of\n",
    "\n",
    "```{math}\n",
    "\\begin{equation} \\label{eq:primal}\n",
    "a_{\\mu}(u_{\\mu}, v) = f_{\\mu}(v) \\qquad \\forall \\, v \\in V \\tag{P.b}.\n",
    "\\end{equation}\n",
    "```\n",
    "\n",
    "The equation {math}`\\eqref{eq:primal}` is called the primal\n",
    "equation and can be arbitrarily complex. MOR methods in the context of\n",
    "PDE-constrained optimization problems thus aim to find a surrogate model\n",
    "of {math}`\\eqref{eq:primal}` to reduce the computational costs of\n",
    "an evaluation of {math}`J(u_{\\mu}, \\mu)`.\n",
    "\n",
    "If there exists a unique solution {math}`u_{\\mu}` for all\n",
    "{math}`\\mu \\in \\mathcal{P}`, we can rewrite (P) by using the so-called\n",
    "reduced objective functional {math}`\\mathcal{J}(\\mu):= J(u_{\\mu}, \\mu)`\n",
    "leading to the equivalent problem: Find a solution of\n",
    "\n",
    "```{math}\n",
    "\\min_{\\mu \\in \\mathcal{P}} \\mathcal{J}(\\mu).  \\tag{$\\hat{P}$}\n",
    "```\n",
    "\n",
    "There exist plenty of different methods to solve ({math}`\\hat{P}`) by\n",
    "using MOR methods. Some of them rely on an RB method with traditional\n",
    "offline/online splitting, which typically result in a very online\n",
    "efficient approach. Recent research also tackles overall efficiency by\n",
    "overcoming the expensive offline phase, which we will discuss further\n",
    "below.\n",
    "\n",
    "In this tutorial, we use a simple linear scalar valued objective functional\n",
    "and an elliptic primal equation to compare different approaches that solve\n",
    "({math}`\\hat{P}`).\n",
    "\n",
    "## An elliptic model problem with a linear objective functional\n",
    "\n",
    "We consider a domain {math}`\\Omega:= [-1, 1]^2`, a parameter set\n",
    "{math}`\\mathcal{P} := [0,\\pi]^2` and the elliptic equation\n",
    "\n",
    "```{math}\n",
    "- \\nabla \\cdot \\big( \\lambda(\\mu) \\nabla u_\\mu \\big) = l\n",
    "```\n",
    "\n",
    "with data functions\n",
    "\n",
    "```{math}\n",
    "\\begin{align}\n",
    "l(x, y) &= \\tfrac{1}{2} \\pi^2 \\cos(\\tfrac{1}{2} \\pi x) \\cos(\\tfrac{1}{2} \\pi y),\\\\\n",
    "\\lambda(\\mu) &= \\theta_0(\\mu) \\lambda_0 + \\theta_1(\\mu) \\lambda_1,\\\\\n",
    "\\theta_0(\\mu) &= 1.1 + \\sin(\\mu_0)\\mu_1,\\\\\n",
    "\\theta_1(\\mu) &= 1.1 + \\sin(\\mu_1),\\\\\n",
    "\\lambda_0 &= \\chi_{\\Omega \\backslash \\omega},\\\\\n",
    "\\lambda_1 &= \\chi_\\omega,\\\\\n",
    "\\omega &:= [-\\tfrac{2}{3}, -\\tfrac{1}{3}]^2 \\cup ([-\\tfrac{2}{3}, -\\tfrac{1}{3}] \\times [\\tfrac{1}{3}, \\tfrac{2}{3}]).\n",
    "\\end{align}\n",
    "```\n",
    "\n",
    "The diffusion is thus given as the linear combination of scaled\n",
    "indicator functions where {math}`\\omega` is defined by two blocks in the\n",
    "left half of the domain, roughly where the `w` is here:\n",
    "\n",
    "```\n",
    "+-----------+\n",
    "|           |\n",
    "|  w        |\n",
    "|           |\n",
    "|  w        |\n",
    "|           |\n",
    "+-----------+\n",
    "```\n",
    "\n",
    "From the definition above we can easily deduce the bilinear form\n",
    "{math}`a_{\\mu}` and the linear functional {math}`f_{\\mu}` for the primal\n",
    "equation. Moreover, we consider the linear objective functional\n",
    "\n",
    "```{math}\n",
    "\n",
    "\\mathcal{J}(\\mu) := \\theta_{\\mathcal{J}}(\\mu)\\, f_\\mu(u_\\mu)\n",
    "\n",
    "```\n",
    "\n",
    "where {math}`\\theta_{\\mathcal{J}}(\\mu) := 1 + \\frac{1}{5}(\\mu_0 + \\mu_1)`.\n",
    "\n",
    "With this data, we can construct a {{ StationaryProblem }} in pyMOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dd081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymor.basic import *\n",
    "import numpy as np\n",
    "\n",
    "domain = RectDomain(([-1,-1], [1,1]))\n",
    "indicator_domain = ExpressionFunction(\n",
    "    '(-2/3. <= x[0]) * (x[0] <= -1/3.) * (-2/3. <= x[1]) * (x[1] <= -1/3.) * 1. \\\n",
    "   + (-2/3. <= x[0]) * (x[0] <= -1/3.) *  (1/3. <= x[1]) * (x[1] <=  2/3.) * 1.',\n",
    "    dim_domain=2)\n",
    "rest_of_domain = ConstantFunction(1, 2) - indicator_domain\n",
    "\n",
    "l = ExpressionFunction('0.5*pi*pi*cos(0.5*pi*x[0])*cos(0.5*pi*x[1])', dim_domain=2)\n",
    "\n",
    "parameters = {'diffusion': 2}\n",
    "thetas = [ExpressionParameterFunctional('1.1 + sin(diffusion[0])*diffusion[1]', parameters,\n",
    "                                       derivative_expressions={'diffusion': ['cos(diffusion[0])*diffusion[1]',\n",
    "                                                                             'sin(diffusion[0])']}),\n",
    "          ExpressionParameterFunctional('1.1 + sin(diffusion[1])', parameters,\n",
    "                                       derivative_expressions={'diffusion': ['0',\n",
    "                                                                             'cos(diffusion[1])']}),\n",
    "\n",
    "                                       ]\n",
    "diffusion = LincombFunction([rest_of_domain, indicator_domain], thetas)\n",
    "\n",
    "theta_J = ExpressionParameterFunctional('1 + 1/5 * diffusion[0] + 1/5 * diffusion[1]', parameters,\n",
    "                                        derivative_expressions={'diffusion': ['1/5','1/5']})\n",
    "\n",
    "problem = StationaryProblem(domain, l, diffusion, outputs=[('l2', l * theta_J)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c48b52",
   "metadata": {},
   "source": [
    "We now use pyMOR's builtin discretization toolkit (see {doc}`tutorial_builtin_discretizer`)\n",
    "to construct a full order {{ StationaryModel }}. Since we intend to use a fixed\n",
    "energy norm\n",
    "\n",
    "```{math}\n",
    "\\|\\,.\\|_{\\bar{\\mu}} : = a_{\\,\\bar{\\mu}}(.,.),\n",
    "```\n",
    "\n",
    "we also define {math}`\\bar{\\mu}`, which we pass via the argument\n",
    "`mu_energy_product`. Also, we define the parameter space\n",
    "{math}`\\mathcal{P}` on which we want to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d5b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_bar = problem.parameters.parse([np.pi/2,np.pi/2])\n",
    "\n",
    "fom, data = discretize_stationary_cg(problem, diameter=1/50, mu_energy_product=mu_bar)\n",
    "parameter_space = fom.parameters.space(0, np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf7b4b9",
   "metadata": {},
   "source": [
    "We now define the first function for the output of the model that will be used by the minimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb7b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fom_objective_functional(mu):\n",
    "    return fom.output(mu)[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f46fd47",
   "metadata": {},
   "source": [
    "We also pick a starting parameter for the optimization method,\n",
    "which in our case is {math}`\\mu^0 = (0.25, 0.5)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4b698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess = [0.25, 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ae6108",
   "metadata": {},
   "source": [
    "Next, we visualize the diffusion function {math}`\\lambda_\\mu` by using\n",
    "{class}`~pymor.discretizers.builtin.cg.InterpolationOperator` for interpolating it on the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aafa02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymor.discretizers.builtin.cg import InterpolationOperator\n",
    "\n",
    "diff = InterpolationOperator(data['grid'], problem.diffusion).as_vector(fom.parameters.parse(initial_guess))\n",
    "fom.visualize(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf20f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['grid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e76d86f",
   "metadata": {},
   "source": [
    "We can see that our FOM model has 20201 DoFs which just about suffices\n",
    "to resolve the data structure in the diffusion. This suggests to use an\n",
    "even finer mesh. However, for enabling a faster runtime for this\n",
    "tutorial, we stick with this mesh and remark that refining the mesh does\n",
    "not change the interpretation of the methods that are discussed below.\n",
    "It rather further improves the speedups achieved by model reduction.\n",
    "\n",
    "Before we discuss the first optimization method, we define helpful\n",
    "functions for visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3941de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (12.0, 8.0)\n",
    "mpl.rcParams['font.size'] = 12\n",
    "mpl.rcParams['savefig.dpi'] = 300\n",
    "mpl.rcParams['figure.subplot.bottom'] = .1\n",
    "mpl.rcParams['axes.facecolor'] = (0.0, 0.0, 0.0, 0.0)\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D # required for 3d plots\n",
    "from matplotlib import cm # required for colors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from time import perf_counter\n",
    "\n",
    "def compute_value_matrix(f, x, y):\n",
    "    f_of_x = np.zeros((len(x), len(y)))\n",
    "    for ii in range(len(x)):\n",
    "        for jj in range(len(y)):\n",
    "            f_of_x[ii][jj] = f((x[ii], y[jj]))\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    return x, y, f_of_x\n",
    "\n",
    "def plot_3d_surface(f, x, y, alpha=1):\n",
    "    X, Y = x, y\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    x, y, f_of_x = compute_value_matrix(f, x, y)\n",
    "    ax.plot_surface(x, y, f_of_x, cmap='Blues',\n",
    "                    linewidth=0, antialiased=False, alpha=alpha)\n",
    "    ax.view_init(elev=27.7597402597, azim=-39.6370967742)\n",
    "    ax.set_xlim3d([-0.10457963, 3.2961723])\n",
    "    ax.set_ylim3d([-0.10457963, 3.29617229])\n",
    "    return ax\n",
    "\n",
    "def addplot_xy_point_as_bar(ax, x, y, color='orange', z_range=None):\n",
    "    ax.plot([y, y], [x, x], z_range if z_range else ax.get_zlim(), color)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1990b9e",
   "metadata": {},
   "source": [
    "Now, we can visualize the objective functional on the parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f33e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = parameter_space.ranges['diffusion']\n",
    "XX = np.linspace(ranges[0] + 0.05, ranges[1], 10)\n",
    "\n",
    "plot_3d_surface(fom_objective_functional, XX, XX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ff556d",
   "metadata": {},
   "source": [
    "Taking a closer look at the functional, we see that it is at least\n",
    "locally convex with a locally unique minimum. In general, however,\n",
    "PDE-constrained optimization problems are not convex. In our case\n",
    "changing the parameter functional {math}`\\theta_{\\mathcal{J}}` can\n",
    "already result in a very different non-convex output functional.\n",
    "\n",
    "In order to record some data during the optimization, we also define\n",
    "helpful functions for recording and reporting the results in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb1daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(offline_time=False, enrichments=False):\n",
    "    data = {'num_evals': 0, 'evaluations' : [], 'evaluation_points': [], 'time': np.inf}\n",
    "    if offline_time:\n",
    "        data['offline_time'] = offline_time\n",
    "    if enrichments:\n",
    "        data['enrichments'] = 0\n",
    "    return data\n",
    "\n",
    "def record_results(function, data, adaptive_enrichment=False, opt_dict=None, mu=None):\n",
    "    if adaptive_enrichment:\n",
    "        # this is for the adaptive case! rom is shiped via the opt_dict argument.\n",
    "        assert opt_dict is not None\n",
    "        QoI, data, rom = function(mu, data, opt_dict)\n",
    "        opt_dict['opt_rom'] = rom\n",
    "    else:\n",
    "        QoI = function(mu)\n",
    "    data['num_evals'] += 1\n",
    "    data['evaluation_points'].append(mu)\n",
    "    data['evaluations'].append(QoI)\n",
    "    return QoI\n",
    "\n",
    "def report(result, data, reference_mu=None):\n",
    "    if (result.status != 0):\n",
    "        print('\\n failed!')\n",
    "    else:\n",
    "        print('\\n succeeded!')\n",
    "        print(f'  mu_min:    {fom.parameters.parse(result.x)}')\n",
    "        print(f'  J(mu_min): {result.fun}')\n",
    "        if reference_mu is not None:\n",
    "            print(f'  absolute error w.r.t. reference solution: {np.linalg.norm(result.x-reference_mu):.2e}')\n",
    "        print(f'  num iterations:     {result.nit}')\n",
    "        print(f'  num function calls: {data[\"num_evals\"]}')\n",
    "        print(f'  time:               {data[\"time\"]:.5f} seconds')\n",
    "        if 'offline_time' in data:\n",
    "                print(f'  offline time:       {data[\"offline_time\"]:.5f} seconds')\n",
    "        if 'enrichments' in data:\n",
    "                print(f'  model enrichments:  {data[\"enrichments\"]}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226f815c",
   "metadata": {},
   "source": [
    "## Optimizing with the FOM using finite differences\n",
    "\n",
    "There exist plenty optimization methods, and this tutorial is not meant\n",
    "to discuss the design and implementation of optimization methods. We\n",
    "simply use the {func}`~scipy.optimize.minimize` function\n",
    "from `scipy.optimize` and use the\n",
    "builtin `L-BFGS-B` routine which is a quasi-Newton method that can\n",
    "also handle a constrained parameter space. For the whole tutorial, we define the\n",
    "optimization function as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065c43dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def optimize(J, data, ranges, gradient=False, adaptive_enrichment=False, opt_dict=None):\n",
    "    tic = perf_counter()\n",
    "    result = minimize(partial(record_results, J, data, adaptive_enrichment, opt_dict),\n",
    "                      initial_guess,\n",
    "                      method='L-BFGS-B', jac=gradient,\n",
    "                      bounds=(ranges, ranges),\n",
    "                      options={'ftol': 1e-15, 'gtol': 5e-5})\n",
    "    data['time'] = perf_counter()-tic\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8a2088",
   "metadata": {},
   "source": [
    "It is optional to give an expression for the gradient of the objective\n",
    "functional to the {func}`~scipy.optimize.minimize` function.\n",
    "In case no gradient is given, {func}`~scipy.optimize.minimize`\n",
    "just approximates the gradient with finite differences.\n",
    "This is not recommended because the gradient is inexact and the\n",
    "computation of finite differences requires even more evaluations of the\n",
    "primal equation. Here, we use this approach for a simple demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464ff143",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_minimization_data = prepare_data()\n",
    "\n",
    "fom_result = optimize(fom_objective_functional, reference_minimization_data, ranges)\n",
    "\n",
    "reference_mu = fom_result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef0f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "report(fom_result, reference_minimization_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a14fe",
   "metadata": {},
   "source": [
    "Taking a look at the result, we see that the optimizer needs {math}`7`\n",
    "iterations to converge, but actually needs {math}`27` evaluations of the\n",
    "full order model. Obviously, this is related to the computation of the\n",
    "finite differences. We can visualize the optimization path by plotting\n",
    "the chosen points during the minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2bf29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_plot = plot_3d_surface(fom_objective_functional, XX, XX, alpha=0.5)\n",
    "\n",
    "for mu in reference_minimization_data['evaluation_points']:\n",
    "    addplot_xy_point_as_bar(reference_plot, mu[0], mu[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a48983f",
   "metadata": {},
   "source": [
    "## Optimizing with the ROM using finite differences\n",
    "\n",
    "We can use a standard RB method to build a surrogate model for the FOM.\n",
    "As a result, the solution of the primal equation is no longer expensive\n",
    "and the optimization method can evaluate the objective functional quickly.\n",
    "For this, we define a standard {class}`~pymor.reductors.coercive.CoerciveRBReductor`\n",
    "and use the {class}`~pymor.parameters.functionals.MinThetaParameterFunctional` for an\n",
    "estimation of the coerciviy constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024a6eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymor.algorithms.greedy import rb_greedy\n",
    "from pymor.parameters.functionals import MinThetaParameterFunctional\n",
    "from pymor.reductors.coercive import CoerciveRBReductor\n",
    "\n",
    "coercivity_estimator = MinThetaParameterFunctional(fom.operator.coefficients, mu_bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38681d3",
   "metadata": {},
   "source": [
    "The online efficiency of MOR methods most likely comes with a\n",
    "rather expensive offline phase. For PDE-constrained optimization, however,\n",
    "it is not meaningful to ignore the\n",
    "offline time of the surrogate model since it can happen that FOM\n",
    "optimization methods would already converge before the surrogate model\n",
    "is even ready. Thus, RB optimization methods (at least for only one\n",
    "configuration) aims for overall efficiency which includes offline and\n",
    "online time. Of course, this effect aggravates if the parameter space is\n",
    "high dimensional because the offline phase can increase even more.\n",
    "\n",
    "In order to decrease the offline time we guess that we may not require\n",
    "a perfect surrogate model in the sense that a low error tolerance for\n",
    "the {func}`~pymor.algorithms.greedy.rb_greedy` already suffices to converge\n",
    "to the same minimum.\n",
    "In our case we choose `atol=1e-2` and yield a very low dimensional space.\n",
    "In general, however, it is not a priori clear how to choose `atol`\n",
    "in order to arrive at a minimum which is close enough to the true\n",
    "optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb60be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = parameter_space.sample_uniformly(25)\n",
    "\n",
    "RB_reductor = CoerciveRBReductor(fom, product=fom.energy_product, coercivity_estimator=coercivity_estimator)\n",
    "RB_greedy_data = rb_greedy(fom, RB_reductor, training_set, atol=1e-2)\n",
    "\n",
    "num_RB_greedy_extensions = RB_greedy_data['extensions']\n",
    "RB_greedy_mus, RB_greedy_errors = RB_greedy_data['max_err_mus'], RB_greedy_data['max_errs']\n",
    "rom = RB_greedy_data['rom']\n",
    "\n",
    "print(f'RB system is of size {num_RB_greedy_extensions}x{num_RB_greedy_extensions}')\n",
    "print(f'maximum estimated model reduction error over training set: {RB_greedy_errors[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7213f2",
   "metadata": {},
   "source": [
    "We can see that greedy algorithm already stops after {math}`3` basis functions.\n",
    "Next, we plot the chosen parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8944d05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_3d_surface(fom_objective_functional, XX, XX, alpha=0.5)\n",
    "\n",
    "for mu in RB_greedy_mus[:-1]:\n",
    "    mu = mu.to_numpy()\n",
    "    addplot_xy_point_as_bar(ax, mu[0], mu[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bc7f0a",
   "metadata": {},
   "source": [
    "Analogously to above, we perform the same optimization method, but use\n",
    "the resulting ROM objective functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8b7627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rom_objective_functional(mu):\n",
    "    return rom.output(mu)[0, 0]\n",
    "\n",
    "RB_minimization_data = prepare_data(offline_time=RB_greedy_data['time'])\n",
    "\n",
    "rom_result = optimize(rom_objective_functional, RB_minimization_data, ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbda8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "report(rom_result, RB_minimization_data, reference_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a58fc6",
   "metadata": {},
   "source": [
    "Comparing the result to the FOM model, we see that the number of\n",
    "iterations and evaluations of the model are equal. As expected,\n",
    "we see that the optmization routine is very fast because the surrogate\n",
    "enables almost instant evaluations of the primal equation.\n",
    "\n",
    "As mentioned above, we should not forget that we required the offline\n",
    "time to build our surrogate. In our case, the offline time is still low\n",
    "enough to get a speed up over the FOM optimization. Luckily,\n",
    "`atol=1e-2` was enough to achieve an absolute error of roughly `1e-06`\n",
    "but it is important to notice that we do not know this error before\n",
    "choosing `atol`.\n",
    "\n",
    "To show that the ROM optimization roughly followed the same path as the\n",
    "FOM optimization, we visualize both of them in the following plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79926d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_plot = plot_3d_surface(fom_objective_functional, XX, XX, alpha=0.5)\n",
    "reference_plot_mean_z_lim = 0.5*(reference_plot.get_zlim()[0] + reference_plot.get_zlim()[1])\n",
    "\n",
    "for mu in reference_minimization_data['evaluation_points']:\n",
    "    addplot_xy_point_as_bar(reference_plot, mu[0], mu[1], color='green',\n",
    "                            z_range=(reference_plot.get_zlim()[0], reference_plot_mean_z_lim))\n",
    "\n",
    "for mu in RB_minimization_data['evaluation_points']:\n",
    "    addplot_xy_point_as_bar(reference_plot, mu[0], mu[1], color='orange',\n",
    "                           z_range=(reference_plot_mean_z_lim, reference_plot.get_zlim()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb86ecb9",
   "metadata": {},
   "source": [
    "## Computing the gradient of the objective functional\n",
    "\n",
    "A major issue of using finite differences for computing the gradient of\n",
    "the objective functional is the number of evaluations of the objective\n",
    "functional. In the FOM example from above we saw that\n",
    "many evaluations of the model were only due to the\n",
    "computation of the finite differences. If the problem is more complex\n",
    "and the mesh is finer, this can lead to a serious waste of\n",
    "computational time. Also from an optimizational point of view it is\n",
    "always better to compute the true gradient of the objective functional.\n",
    "\n",
    "For computing the gradient of the linear objective functional\n",
    "{math}`\\mathcal{J}(\\mu)`, we can write for every direction\n",
    "{math}`i= 1, \\dots, P`\n",
    "\n",
    "```{math}\n",
    "\\begin{align} \\label{gradient:sens} \\tag{1}\n",
    "d_{\\mu_i} \\mathcal{J}(\\mu) = \\partial_{\\mu_i} J(u_{\\mu}, \\mu) + \\partial_u J(u_{\\mu}, \\mu)[d_{\\mu_i} u_{\\mu}]\n",
    "   =   \\partial_{\\mu_i} J(u_{\\mu}, \\mu) + J(d_{\\mu_i} u_{\\mu}, \\mu)\n",
    "\\end{align}\n",
    "```\n",
    "\n",
    "Thus, we need to compute the derivative of the\n",
    "solution {math}`u_{\\mu}` (also called sensitivity). For this, we need to\n",
    "solve another equation: Find {math}`d_{\\mu_i} u_{\\mu} \\in V`, such that\n",
    "\n",
    "```{math}\n",
    " \\label{sens} \\tag{2}\n",
    "a_\\mu(d_{\\mu_i} u_{\\mu}, v) = \\partial_{\\mu_i} r_\\mu^{\\text{pr}}(u_{\\mu})[v] \\qquad \\qquad \\forall v \\in V\n",
    "```\n",
    "\n",
    "where {math}`r_\\mu^{\\text{pr}}` denotes the residual of the primal\n",
    "equation, i.e.\n",
    "\n",
    "```{math}\n",
    "r_\\mu^{\\text{pr}}(u)[v] := l_\\mu(v) - a_\\mu(u, v) \\qquad \\qquad \\text{for all }v \\in V\n",
    "```\n",
    "\n",
    "A major issue of this approach is that the computation of the\n",
    "full gradient requires {math}`P` solutions of {math}`\\eqref{sens}`.\n",
    "Especially for high dimensional parameter spaces, we can instead use an\n",
    "adjoint approach to reduce the computational cost to only one solution\n",
    "of an additional problem.\n",
    "\n",
    "The adjoint approach relies on the Lagrangian of the objective\n",
    "functional\n",
    "\n",
    "```{math}\n",
    "\\mathcal{L}(u, \\mu, p) = J(u, \\mu) + r_\\mu^{\\text{pr}}(u, p)\n",
    "```\n",
    "\n",
    "where {math}`p \\in V` is the adjoint variable. Deriving optimality\n",
    "conditions for {math}`\\mathcal{L}`, we end up with the dual equation:\n",
    "Find {math}`p_{\\mu} \\in V`, such that\n",
    "\n",
    "```{math}\n",
    " \\label{dual} \\tag{3}\n",
    "a_\\mu(v, p_\\mu) = \\partial_u J(u_\\mu, \\mu)[v]\n",
    "= J(v, \\mu)\n",
    "```\n",
    "\n",
    "Note that in our case, we then have\n",
    "{math}`\\mathcal{L}(u_{\\mu}, \\mu, p_{\\mu}) = J(u, \\mu)` because the\n",
    "residual term {math}`r_\\mu^{\\text{pr}}(u_{\\mu}, p_{\\mu})` vanishes since {math}`u_{\\mu}`\n",
    "solves {math}`\\eqref{eq:primal}` and {math}`p_{\\mu}` is in the test space {math}`V`. By\n",
    "using the solution of the dual problem, we can then derive the gradient of the objective\n",
    "functional by\n",
    "\n",
    "```{math}\n",
    "\\begin{align}\n",
    "d_{\\mu_i} \\mathcal{J}(\\mu) &= \\partial_{\\mu_i} J(u_{\\mu}, \\mu) + \\partial_u J(u_{\\mu}, \\mu)[d_{\\mu_i} u_{\\mu}] \\\\\n",
    "   &=   \\partial_{\\mu_i} J(u_{\\mu}, \\mu) + a_\\mu(d_{\\mu_i} u_{\\mu}, p_\\mu) \\\\\n",
    "   &=   \\partial_{\\mu_i} J(u_{\\mu}, \\mu) + \\partial_{\\mu_i} r_\\mu^{\\text{pr}}(d_{\\mu_i} u_{\\mu})[p_\\mu]\n",
    "\\end{align}\n",
    "```\n",
    "\n",
    "We conclude that we only need to solve for {math}`u_{\\mu}` and\n",
    "{math}`p_{\\mu}` if we want to compute the gradient with the adjoint\n",
    "approach. For more information on this approach we refer to Section 1.6.2 in {cite}`HPUU09`.\n",
    "\n",
    "We now intend to use the gradient to speed up the optimization methods\n",
    "from above. All technical requirements are\n",
    "already available in pyMOR.\n",
    "\n",
    "## Optimizing using a gradient in FOM\n",
    "\n",
    "We can easily include a function to compute the gradient to {func}`~scipy.optimize.minimize`.\n",
    "Since we use a linear operator and a linear objective functional, the `use_adjoint` argument\n",
    "is automatically enabled.\n",
    "Note that using the (more general) implementation `use_adjoint=False` results\n",
    "in the exact same gradient but lacks computational speed.\n",
    "Moreover, the function `output_d_mu` returns a dict w.r.t. the parameters as default.\n",
    "In order to use the output for {func}`~scipy.optimize.minimize` we thus use the `return_array=True` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e779fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fom_gradient_of_functional(mu):\n",
    "    return fom.output_d_mu(fom.parameters.parse(mu), return_array=True, use_adjoint=True)\n",
    "\n",
    "opt_fom_minimization_data = prepare_data()\n",
    "\n",
    "opt_fom_result = optimize(fom_objective_functional, opt_fom_minimization_data, ranges,\n",
    "                          gradient=fom_gradient_of_functional)\n",
    "\n",
    "# update the reference_mu because this is more accurate!\n",
    "reference_mu = opt_fom_result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0814789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "report(opt_fom_result, opt_fom_minimization_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f02546f",
   "metadata": {},
   "source": [
    "With respect to the FOM result with finite differences, we see that we\n",
    "have a saved the evaluations for computing the gradient. Of course it is also not for free to\n",
    "compute the gradient, but since we are using the dual approach, this will only scale with the\n",
    "factor 2. Furthermore, we can expect that the result above is more accurate which is why we\n",
    "choose it as the reference parameter.\n",
    "\n",
    "## Optimizing using a gradient in ROM\n",
    "\n",
    "Obviously, we can also include the gradient of the ROM version of the\n",
    "output functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de6a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rom_gradient_of_functional(mu):\n",
    "    return rom.output_d_mu(rom.parameters.parse(mu), return_array=True, use_adjoint=True)\n",
    "\n",
    "opt_rom_minimization_data = prepare_data(offline_time=RB_greedy_data['time'])\n",
    "\n",
    "opt_rom_result = optimize(rom_objective_functional, opt_rom_minimization_data, ranges,\n",
    "                          gradient=rom_gradient_of_functional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1836c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "report(opt_rom_result, opt_rom_minimization_data, reference_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c106b6c6",
   "metadata": {},
   "source": [
    "The online phase is even faster but the offline time of course remains the same.\n",
    "We also conclude that the ROM model eventually gives less speedup by using a better optimization\n",
    "method for the FOM and ROM.\n",
    "\n",
    "## Beyond the traditional offline/online splitting: enrich along the path of optimization\n",
    "\n",
    "We already figured out that the main drawback for using RB methods in the\n",
    "context of optimization is the expensive offline time to build the\n",
    "surrogate model. In the example above, we overcame this issue by\n",
    "choosing a large tolerance `atol`. As a result, we cannot be sure\n",
    "that our surrogate model is accurate enough for our purpuses. In other\n",
    "words, either we invest too much time to build an accurate model or we\n",
    "face the danger of reducing with a bad surrogate for the whole parameter\n",
    "space. Thinking about this issue again, it is important to notice that\n",
    "we are solving an optimization problem which will eventually converge to\n",
    "a certain parameter. Thus, it only matters that the surrogate is good in\n",
    "this particular region as long as we are able to arrive at it. This\n",
    "gives hope that there must exist a more efficient way of using RB\n",
    "methods without trying to approximate the FOM across the\n",
    "whole parameter space.\n",
    "\n",
    "One possible way for advanced RB methods is a reduction along the path\n",
    "of optimization. The idea is that we start with an empty basis and only\n",
    "enrich the model with the parameters that we will arive at. This\n",
    "approach goes beyond the classical offline/online splitting of RB\n",
    "methods since it entirely skips the offline phase. In the following\n",
    "code, we will test this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a932977",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdeopt_reductor = CoerciveRBReductor(\n",
    "    fom, product=fom.energy_product, coercivity_estimator=coercivity_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5f3716",
   "metadata": {},
   "source": [
    "In the next function, we implement the above mentioned way of enriching\n",
    "the basis along the path of optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3810ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_and_compute_objective_function(mu, data, opt_dict):\n",
    "    U = fom.solve(mu)\n",
    "    try:\n",
    "        pdeopt_reductor.extend_basis(U)\n",
    "        data['enrichments'] += 1\n",
    "    except:\n",
    "        print('Extension failed')\n",
    "    opt_rom = pdeopt_reductor.reduce()\n",
    "    QoI = opt_rom.output(mu)\n",
    "    return QoI, data, opt_rom\n",
    "\n",
    "def compute_gradient_with_opt_rom(opt_dict, mu):\n",
    "    opt_rom = opt_dict['opt_rom']\n",
    "    return opt_rom.output_d_mu(opt_rom.parameters.parse(mu), return_array=True, use_adjoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36f76ab",
   "metadata": {},
   "source": [
    "With this definitions, we can start the optimization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fe2437",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_along_path_minimization_data = prepare_data(enrichments=True)\n",
    "\n",
    "opt_dict = {}\n",
    "opt_along_path_result = optimize(enrich_and_compute_objective_function,\n",
    "                                 opt_along_path_minimization_data, ranges,\n",
    "                                 gradient=partial(compute_gradient_with_opt_rom, opt_dict),\n",
    "                                 adaptive_enrichment=True, opt_dict=opt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "report(opt_along_path_result, opt_along_path_minimization_data, reference_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91f3e2c",
   "metadata": {},
   "source": [
    "The computational time looks at least better than the FOM optimization\n",
    "and we are very close to the reference parameter.\n",
    "But we are following the exact same path than the\n",
    "FOM and thus we need to solve the FOM model as often as before\n",
    "(due to the enrichments). The only computational time that we safe is the one\n",
    "for the gradients since we compute the dual solutions with the ROM.\n",
    "\n",
    "## Adaptively enriching along the path\n",
    "\n",
    "In order to further speedup the above algorithm, we enhance it\n",
    "by only adaptive enrichments of the model.\n",
    "For instance it may happen that the model is already good at\n",
    "the next iteration, which we can easily check by evaluating the standard\n",
    "error estimator which is also used in the greedy algorithm. In the next\n",
    "example we will implement this adaptive way of enriching and set a\n",
    "tolerance which is equal to the one that we had as error tolerance\n",
    "in the greedy algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e43d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdeopt_reductor = CoerciveRBReductor(\n",
    "    fom, product=fom.energy_product, coercivity_estimator=coercivity_estimator)\n",
    "opt_rom = pdeopt_reductor.reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd483ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_adaptively_and_compute_objective_function(mu, data, opt_dict):\n",
    "    opt_rom = opt_dict['opt_rom']\n",
    "    primal_estimate = opt_rom.estimate_error(opt_rom.parameters.parse(mu))\n",
    "    if primal_estimate > 1e-2:\n",
    "        print('Enriching the space because primal estimate is {} ...'.format(primal_estimate))\n",
    "        U = fom.solve(mu)\n",
    "        try:\n",
    "            pdeopt_reductor.extend_basis(U)\n",
    "            data['enrichments'] += 1\n",
    "            opt_rom = pdeopt_reductor.reduce()\n",
    "        except:\n",
    "            print('... Extension failed')\n",
    "    else:\n",
    "        print('Do NOT enrich the space because primal estimate is {} ...'.format(primal_estimate))\n",
    "    opt_rom = pdeopt_reductor.reduce()\n",
    "    QoI = opt_rom.output(mu)[0, 0]\n",
    "    return QoI, data, opt_rom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b968b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_along_path_adaptively_minimization_data = prepare_data(enrichments=True)\n",
    "\n",
    "opt_dict = {'opt_rom': opt_rom}\n",
    "opt_along_path_adaptively_result = optimize(enrich_adaptively_and_compute_objective_function,\n",
    "                                            opt_along_path_adaptively_minimization_data, ranges,\n",
    "                                            gradient=partial(compute_gradient_with_opt_rom, opt_dict),\n",
    "                                            adaptive_enrichment=True, opt_dict=opt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d98086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "report(opt_along_path_adaptively_result, opt_along_path_adaptively_minimization_data, reference_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9e180",
   "metadata": {},
   "source": [
    "Now, we actually only needed {math}`4` enrichments and ended up with an\n",
    "approximation error of about `1e-07` while getting the highest speed up\n",
    "amongst all methods that we have seen above. Note, however, that this is\n",
    "still dependent on the tolerance `atol=1e-2` that we chose without\n",
    "knowing that this tolerance suffices to reach the actual minimum.\n",
    "An easy way around this would be to do one optimization step with the FOM\n",
    "after converging. If this changes anything, the ROM tolerance `atol`\n",
    "was too large. To conclude, we once again\n",
    "compare all methods that we have discussed in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf23343",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('FOM with finite differences')\n",
    "report(fom_result, reference_minimization_data, reference_mu)\n",
    "\n",
    "print('\\nROM with finite differences')\n",
    "report(rom_result, RB_minimization_data, reference_mu)\n",
    "\n",
    "print('\\nFOM with gradient')\n",
    "report(opt_fom_result, opt_fom_minimization_data, reference_mu)\n",
    "\n",
    "print('\\nROM with gradient')\n",
    "report(opt_rom_result, opt_rom_minimization_data, reference_mu)\n",
    "\n",
    "print('\\nAlways enrich along the path')\n",
    "report(opt_along_path_result, opt_along_path_minimization_data, reference_mu)\n",
    "\n",
    "print('\\nAdaptively enrich along the path')\n",
    "report(opt_along_path_adaptively_result, opt_along_path_adaptively_minimization_data, reference_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dd4866",
   "metadata": {
    "tags": [
     "hide-code",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "assert fom_result.nit == 7\n",
    "assert opt_along_path_result.nit == 7\n",
    "assert opt_along_path_minimization_data['num_evals'] == 9\n",
    "assert opt_along_path_minimization_data['enrichments'] == 9\n",
    "assert opt_along_path_adaptively_minimization_data['enrichments'] == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0953dc1c",
   "metadata": {},
   "source": [
    "## Conclusion and some general words about MOR methods for optimization\n",
    "\n",
    "In this tutorial we have seen how pyMOR can be used to speedup the optimizer\n",
    "for PDE-constrained optimization problems.\n",
    "We focused on several aspects of RB methods and showed how explicit gradient information\n",
    "helps to reduce the computational cost of the optimizer.\n",
    "We also saw that already standard RB methods may help to reduce the computational time.\n",
    "It is clear that standard RB methods are especially of interest if an\n",
    "optimization problem needs to be solved multiple times.\n",
    "\n",
    "Moreover, we focused on the lack of overall efficiency of standard RB methods.\n",
    "To overcome this, we reduced the (normally) expensive offline time by choosing larger\n",
    "tolerances for the greedy algorithm.\n",
    "We have also seen a way to overcome\n",
    "the traditional offline/online splitting by only enriching the model along\n",
    "the path of optimization or (even better) only enrich\n",
    "the model if the standard error estimator goes above a certain tolerance.\n",
    "\n",
    "In this tutorial we have only covered a few basic approaches to combine model\n",
    "reduction with optimization.\n",
    "For faster and more robust optimization algorithms we refer to the textbooks\n",
    "[CGT00](<https://epubs.siam.org/doi/book/10.1137/1.9780898719857>) and\n",
    "[NW06](<https://link.springer.com/book/10.1007/978-0-387-40065-5>).\n",
    "For recent research on combining trust-region methods with model reduction for\n",
    "PDE-constrained optimization problems we refer to\n",
    "[YM13](<https://epubs.siam.org/doi/abs/10.1137/120869171>),\n",
    "[QGVW17](<https://epubs.siam.org/doi/abs/10.1137/16M1081981>) and\n",
    "[KMSOV20](<https://arxiv.org/abs/2006.09297>) where for the latter a pyMOR\n",
    "implementation is available as supplementary material.\n",
    "\n",
    "Download the code:\n",
    "{download}`tutorial_optimization.md`\n",
    "{nb-download}`tutorial_optimization.ipynb`"
   ]
  }
 ],
 "metadata": {
  "jupyter": {
   "jupytext": {
    "cell_metadata_filter": "-all",
    "formats": "ipynb,myst",
    "main_language": "python",
    "text_representation": {
     "extension": ".md",
     "format_name": "myst",
     "format_version": "1.3",
     "jupytext_version": "1.11.2"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
